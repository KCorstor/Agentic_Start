{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daab331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevincorstorphine/Desktop/Agentic_Start\n",
      "Loading .env file from: /Users/kevincorstorphine/Desktop/Agentic_Start/.env\n"
     ]
    }
   ],
   "source": [
    "# GET LIST FROM BDR. CRAFT A COLD OUTREACH EMAIL TO CONTACT 1 OF EACH COMPANY BASED ON COMPANY DATA (SHOULD BE IN CHATGPT) AND PCIO DATA\n",
    "# DATA SHOULD ALREADY BE IN HUBSPOT AT THIS POINT. \n",
    "\n",
    "#GIVE THE CLIENT SOMETHING. EITHER SHOW YOU KNOW ABOUT THEIR BUSINESS, SHOW YOU KNOW THEIR PAIN POINTS, SHOW YOU KNOW SOMETHING. KEEP EMAIL SHORT SO IT DOESNT LOOK LIKE AI.\n",
    "\n",
    "# THE TRAINING AND GENERATING A NOT SHITTY COLD EMAIL IS THE PRIMARY OBJECTIVE HERE. \n",
    "\n",
    "\n",
    "\n",
    "#All of the dependencies are listed in the .toml file. Ensure those are going with Poetry\n",
    "#ensure your .env file is in the right place \n",
    "\n",
    "#make sure you're using poetry shell to get all of the dependencies going in your folder\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(os.getcwd())\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "creativeLlmModel = OpenAI(temperature=0.9)\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0006a-925a-475d-bf4a-3faf78ae322f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f340c7-4140-4569-8bb9-afa9f88723f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c95b7-591d-499b-b382-07500fffbcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80da6210-ec6b-48bd-9198-e4db3709e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"model_background_personality.txt\")\n",
    "scrape_data1 = TextLoader(\"scraped_pages/1example_com_combined.txt\")\n",
    "\n",
    "loaded_data = loader.load()\n",
    "scrape_data1 = scrape_data1.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a56367f8-01e5-49b4-be93-71c09076e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need smaller chunks, you can enforce a maximum chunk size\n",
    "#to ensure that the splitter splits. It looks for logical places to split\n",
    "#and the .txt scrapes are jumbled and don't have logical breaks.\n",
    "\n",
    "# It may make sense to \n",
    "\n",
    "\n",
    "#Splitters to call small sections of relevant data, but it is also limiting in how to craft a creative response AFAIK\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "434a3fbe-353f-4cf6-bb0d-8fecfd16d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4565, which is longer than the specified 1000\n",
      "Created a chunk of size 4565, which is longer than the specified 1000\n",
      "Created a chunk of size 1711, which is longer than the specified 1000\n",
      "Created a chunk of size 3141, which is longer than the specified 1000\n",
      "Created a chunk of size 3098, which is longer than the specified 1000\n",
      "Created a chunk of size 1360, which is longer than the specified 1000\n",
      "Created a chunk of size 2155, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1738, which is longer than the specified 1000\n",
      "Created a chunk of size 27868, which is longer than the specified 1000\n",
      "Created a chunk of size 1730, which is longer than the specified 1000\n",
      "Created a chunk of size 29895, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "load = text_splitter.create_documents([loaded_data[0].page_content])\n",
    "scrape1 = text_splitter.create_documents([scrape_data1[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0302e62-251c-4145-ba11-56e258693f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load)\n",
    "len(scrape1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63417ba-9f73-4379-99ef-8c4c45cdb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO BETTER PROMPTS READ THE E-BOOK ON PROMPTING\n",
    "\n",
    "\n",
    "#I may need to use vector databases here, or maybe not. not totally sure yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fde5872-48d3-4fba-950d-991bb6ecb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loaded_data is the personality I gave the model in the model_background_personality.txt file. I told it to be a pirate. In the live version\n",
    "#change this to the background about API3 and the product. \n",
    "\n",
    "#it is a pirate doing cold outreach selling bananas. \n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {kind_of_entity}.\"),\n",
    "        (\"human\", \"Hello, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}{scrape_detail}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    kind_of_entity=str(load),\n",
    "   # topic=\"pirating\",\n",
    "    user_input= \"Please write an outbound cold email about the problems they may be facing without a reliable helpdesk IT support service\",\n",
    "    scrape_detail=str(scrape1),\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e7f9de-494b-4a62-8607-a7118317a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616538c-8855-45c8-bb86-e1994a266e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9eaad99-9768-4cc1-93b6-d080f02732f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Is Your IT Support Holding You Back?\n",
      "\n",
      "Hi [Recipient's Name],\n",
      "\n",
      "I hope this message finds you well. I wanted to reach out because many businesses today face challenges that can significantly impact their productivity and bottom line, especially when it comes to IT support.\n",
      "\n",
      "Are you experiencing any of the following issues?\n",
      "\n",
      "- Frequent downtime that disrupts operations and frustrates your team?\n",
      "- Slow response times when technical problems arise, leading to lost productivity?\n",
      "- Difficulty in managing IT resources effectively, causing delays in critical projects?\n",
      "- Security concerns that keep you up at night, knowing your systems may not be adequately protected?\n",
      "\n",
      "At PCIO, we understand that a reliable helpdesk IT support service can be a game-changer. Our dedicated team focuses on providing solutions tailored to your needs, helping you overcome these challenges and ensuring your business runs smoothly.\n",
      "\n",
      "If any of these problems resonate with you, I would love to have a conversation to explore how we can assist you in enhancing your IT support and improving your overall operational efficiency.\n",
      "\n",
      "Looking forward to hearing from you!\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "PCIO  \n",
      "[Your Contact Information]  \n",
      "[Your Website]  \n"
     ]
    }
   ],
   "source": [
    "print(response.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f7a773-9186-40ed-9317-00d19580265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "#examples are a way to train the model to react exactly how you want it to.\n",
    "#in the class example, this was is way to get it to speak Spanish\n",
    "#FewshotChatprompttemplate is just chatPromptTemplate with examples\n",
    "#Use this further modeling to refine the pitch, say waht works, what doesn't, etc.\n",
    "\n",
    "#train AI on how you want an outbound email to look. These are just two examples of how to train it:\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" you are only targeting Spanish speaking communities and are translating to spanish\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#a chain defines a sequence of order in which to run\n",
    "\n",
    "\n",
    "chain = final_prompt | chatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb550660-4b38-4f78-a3b4-3a2d44d75dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Imaginemos que estoy vendiendo un purificador de aire.\n",
      "\n",
      "---\n",
      "\n",
      "¡Hola! ¿Te has dado cuenta de cuánto tiempo pasamos en interiores? La calidad del aire que respiramos en casa o en la oficina puede afectar nuestra salud y bienestar. Te presento nuestro purificador de aire de última generación.\n",
      "\n",
      "Este purificador no solo elimina el 99.9% de los alérgenos y contaminantes del aire, como el polvo, el polen y el moho, sino que también cuenta con un sistema de filtración HEPA que garantiza un ambiente más saludable para ti y tu familia. \n",
      "\n",
      "Además, su diseño elegante se adapta a cualquier espacio, y su funcionamiento es tan silencioso que apenas notarás que está trabajando. También cuenta con un temporizador y modos de funcionamiento que puedes ajustar según tus necesidades.\n",
      "\n",
      "Imagina respirar aire fresco y limpio todos los días. ¡Haz la inversión en tu salud y bienestar hoy! Con cada compra, te ofrecemos un 20% de descuento durante este mes. No dejes pasar esta oportunidad. \n",
      "\n",
      "¿Qué dices? ¿Te gustaría saber más sobre cómo mejorar la calidad del aire en tu hogar?\n",
      "\n",
      "--- \n",
      "\n",
      "¿Te gustaría que ajuste el enfoque o el producto?\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"input\": \"please do a sales pitch to someone based on the product you sell\"}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32eea17-3cc9-4c03-bb6e-4b579fbc8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ PROMPTING BOOK AND DIAL THIS ALL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "475c525b-f1b9-4954-9034-728b7c67de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# FOR ADDITIONAL JSON PARSING STUFF< CHECK OUT THE PYDANTIC LIBRARY\n",
    "\n",
    "\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "# json_prompt = PromptTemplate.from_template(\n",
    "#     \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    "# )\n",
    "\n",
    "# json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2486f803-35dd-4fa8-b76a-c770b5fc16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b8a95b-5b40-4617-9367-29024f643ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_chain.invoke({\"question\": \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27939aa-c158-4b7f-bcbb-9d93b21df195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dadc2f4-08ec-4737-8f17-74fc1314fe25",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (3837994639.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    MAKE SOME GRAPHIC BASED ON WHAT THEY'RE LIKELY SPENDIN, CUSTOMIZE IT, AND SEND THEM A NEW ONE WITH HOW PCIO SOLVES THEIR PROBLEM\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e1b5d2-2123-4f04-843c-b7fcc0af4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE SOME GRAPHIC BASED ON WHAT THEY'RE LIKELY SPENDING, CUSTOMIZE IT, AND SEND THEM A NEW ONE WITH HOW PCIO SOLVES THEIR PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65f6378-03b8-4041-a1bd-9c94b2e9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mom, research Klayvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2287bca8-9405-4595-b3aa-9d1bf0a7c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e12a893a-81c8-4b17-932d-83a0ff629b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed06f9-d74b-4e7a-8a14-437d5cb9e413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
