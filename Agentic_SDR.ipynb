{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daab331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevincorstorphine/Desktop/Agentic_Start\n",
      "Loading .env file from: /Users/kevincorstorphine/Desktop/Agentic_Start/.env\n"
     ]
    }
   ],
   "source": [
    "#Will need to create a vector database that has all of PCIO's training material on it. This ensures the LLM can write cold outreach, but actually knows what PCIO is all about. \n",
    "#A more advanced version of this would include training the vector db on all of dad's emails. Will probably need to do vector db classes over again\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA SHOULD ALREADY BE IN HUBSPOT AT THIS POINT. \n",
    "#Check Hubspot and see when last outreach was. if never, then craft outreach.\n",
    "#WHAT IS MATT'S SWARM METHOD?\n",
    "#send 3 weeks to first lead, then if no response, send to second lead,3 weeks, then third week \n",
    "\n",
    "    \n",
    "#GIVE THE CLIENT SOMETHING. EITHER SHOW YOU KNOW ABOUT THEIR BUSINESS, SHOW YOU KNOW THEIR PAIN POINTS, SHOW YOU KNOW SOMETHING. KEEP EMAIL SHORT SO IT DOESNT LOOK LIKE AI.\n",
    "\n",
    "# THE TRAINING AND GENERATING A NOT SHITTY COLD EMAIL IS THE PRIMARY OBJECTIVE HERE. \n",
    "\n",
    "# Generate a graphic of what PCIO helpdesk could look like, and show how their setup may currently be. Show the cost savings.\n",
    "\n",
    "#All of the dependencies are listed in the .toml file. Ensure those are going with Poetry\n",
    "#ensure your .env file is in the right place \n",
    "\n",
    "#make sure you're using poetry shell to get all of the dependencies going in your folder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example outbound. Do an Analysis of the company, amount of people, no tech people on staff, x size, probably spending around x. If that's anywhere near correct, we can save you money. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(os.getcwd())\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "creativeLlmModel = OpenAI(temperature=0.9)\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0006a-925a-475d-bf4a-3faf78ae322f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f340c7-4140-4569-8bb9-afa9f88723f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c95b7-591d-499b-b382-07500fffbcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80da6210-ec6b-48bd-9198-e4db3709e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"model_background_personality.txt\")\n",
    "scrape_data1 = TextLoader(\"scraped_pages/example_com_combined.txt\")\n",
    "\n",
    "loaded_data = loader.load()\n",
    "scrape_data1 = scrape_data1.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a56367f8-01e5-49b4-be93-71c09076e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need smaller chunks, you can enforce a maximum chunk size\n",
    "# to ensure that the splitter splits. It looks for logical places to split\n",
    "# and the .txt scrapes are jumbled and don't have logical breaks.\n",
    "\n",
    "# It may make sense to \n",
    "\n",
    "\n",
    "# Splitters to call small sections of relevant data, but it is also limiting in how to craft a creative response AFAIK\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "434a3fbe-353f-4cf6-bb0d-8fecfd16d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4565, which is longer than the specified 1000\n",
      "Created a chunk of size 4565, which is longer than the specified 1000\n",
      "Created a chunk of size 1711, which is longer than the specified 1000\n",
      "Created a chunk of size 3141, which is longer than the specified 1000\n",
      "Created a chunk of size 3098, which is longer than the specified 1000\n",
      "Created a chunk of size 1360, which is longer than the specified 1000\n",
      "Created a chunk of size 2155, which is longer than the specified 1000\n",
      "Created a chunk of size 1037, which is longer than the specified 1000\n",
      "Created a chunk of size 1738, which is longer than the specified 1000\n",
      "Created a chunk of size 27868, which is longer than the specified 1000\n",
      "Created a chunk of size 1730, which is longer than the specified 1000\n",
      "Created a chunk of size 29895, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "load = text_splitter.create_documents([loaded_data[0].page_content])\n",
    "scrape1 = text_splitter.create_documents([scrape_data1[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0302e62-251c-4145-ba11-56e258693f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(load)\n",
    "# len(scrape1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86ab0c44-c783-4880-bba9-91cf43957c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW LCEL\n",
    "\n",
    "\n",
    "#User's Input > Prompt Template > LLM Model > Output Parser> Final Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e63417ba-9f73-4379-99ef-8c4c45cdb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO BETTER PROMPTS READ THE E-BOOK ON PROMPTING\n",
    "\n",
    "\n",
    "#I may need to use vector databases here, or maybe not. not totally sure yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fde5872-48d3-4fba-950d-991bb6ecb8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Subject: Are You Facing IT Support Challenges?\\n\\nHi [Recipient's Name],\\n\\nI hope this message finds you well. \\n\\nIn today's fast-paced business environment, having a reliable IT support service is more crucial than ever. Without it, you might be encountering challenges such as prolonged downtime, decreased productivity, and frustrated employees. Each of these issues can lead to a negative impact on your bottom line and customer satisfaction.\\n\\nImagine a scenario where your team faces a technical issue, and without immediate support, projects are stalled, deadlines are missed, and stress levels rise. This not only affects your operations but could also tarnish your reputation.\\n\\nAt PCIO, we understand the importance of seamless IT operations. We specialize in providing robust helpdesk support that addresses these challenges head-on. Our services ensure that your team can focus on their core tasks without the distraction of unresolved technical issues.\\n\\nI would love to discuss how we can help you navigate these challenges effectively. Would you be open to a brief conversation this week?\\n\\nLooking forward to hearing from you.\\n\\nBest regards,\\n\\n[Your Name]  \\n[Your Position]  \\nPCIO  \\n[Your Contact Information]  \" response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 25352, 'total_tokens': 25579, 'prompt_tokens_details': {'cached_tokens': 25216}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None} id='run-131e476b-d382-4ebd-b9dc-192674559655-0' usage_metadata={'input_tokens': 25352, 'output_tokens': 227, 'total_tokens': 25579}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Loaded_data is the personality I gave the model in the model_background_personality.txt file. I told it to be a pirate. In the live version\n",
    "#change this to the background about API3 and the product. \n",
    "\n",
    "#it is a pirate doing cold outreach selling bananas. \n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {kind_of_entity}.\"),\n",
    "        (\"human\", \"Hello, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}{scrape_detail}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    kind_of_entity=str(load),\n",
    "   # topic=\"pirating\",\n",
    "    user_input= \"Please write an outbound cold email about the problems they may be facing without a reliable helpdesk IT support service\",\n",
    "    scrape_detail=str(scrape1),\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b9506-6d4d-4fb7-b333-65bea354e9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7f9de-494b-4a62-8607-a7118317a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616538c-8855-45c8-bb86-e1994a266e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eaad99-9768-4cc1-93b6-d080f02732f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f7a773-9186-40ed-9317-00d19580265a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 40\u001b[0m\n\u001b[1;32m     29\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m     30\u001b[0m     [\n\u001b[1;32m     31\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m you are only targeting Spanish speaking communities and are translating to spanish\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     ]\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#a chain defines a sequence of order in which to run\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m chain \u001b[38;5;241m=\u001b[39m final_prompt \u001b[38;5;241m|\u001b[39m chatModel \u001b[38;5;241m|\u001b[39m \u001b[43moutput_parser\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_parser' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#examples are a way to train the model to react exactly how you want it to.\n",
    "#in the class example, this was is way to get it to speak Spanish\n",
    "#FewshotChatprompttemplate is just chatPromptTemplate with examples\n",
    "#Use this further modeling to refine the pitch, say waht works, what doesn't, etc.\n",
    "\n",
    "#train AI on how you want an outbound email to look. These are just two examples of how to train it:\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" you are only targeting Spanish speaking communities and are translating to spanish\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#a chain defines a sequence of order in which to run\n",
    "\n",
    "\n",
    "chain = final_prompt | chatModel | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb550660-4b38-4f78-a3b4-3a2d44d75dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"input\": \"please do a sales pitch to someone based on the product you sell\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32eea17-3cc9-4c03-bb6e-4b579fbc8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ PROMPTING BOOK AND DIAL THIS ALL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "475c525b-f1b9-4954-9034-728b7c67de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# FOR ADDITIONAL JSON PARSING STUFF< CHECK OUT THE PYDANTIC LIBRARY\n",
    "\n",
    "\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "# json_prompt = PromptTemplate.from_template(\n",
    "#     \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    "# )\n",
    "\n",
    "# json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2486f803-35dd-4fa8-b76a-c770b5fc16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b8a95b-5b40-4617-9367-29024f643ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_chain.invoke({\"question\": \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27939aa-c158-4b7f-bcbb-9d93b21df195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadc2f4-08ec-4737-8f17-74fc1314fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For opt-out list, do opt out button or whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e1b5d2-2123-4f04-843c-b7fcc0af4b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65f6378-03b8-4041-a1bd-9c94b2e9e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2287bca8-9405-4595-b3aa-9d1bf0a7c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e12a893a-81c8-4b17-932d-83a0ff629b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed06f9-d74b-4e7a-8a14-437d5cb9e413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
