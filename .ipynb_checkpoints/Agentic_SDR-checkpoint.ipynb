{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daab331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevincorstorphine/Desktop/Agentic_Start\n",
      "Loading .env file from: /Users/kevincorstorphine/Desktop/Agentic_Start/.env\n"
     ]
    }
   ],
   "source": [
    "# GET LIST FROM BDR. CRAFT A COLD OUTREACH EMAIL TO CONTACT 1 OF EACH COMPANY BASED ON COMPANY DATA (SHOULD BE IN CHATGPT) AND PCIO DATA\n",
    "# DATA SHOULD ALREADY BE IN HUBSPOT AT THIS POINT. \n",
    "\n",
    "#GIVE THE CLIENT SOMETHING. EITHER SHOW YOU KNOW ABOUT THEIR BUSINESS, SHOW YOU KNOW THEIR PAIN POINTS, SHOW YOU KNOW SOMETHING. KEEP EMAIL SHORT SO IT DOESNT LOOK LIKE AI.\n",
    "\n",
    "# THE TRAINING AND GENERATING A NOT SHITTY COLD EMAIL IS THE PRIMARY OBJECTIVE HERE. \n",
    "\n",
    "\n",
    "\n",
    "#All of the dependencies are listed in the .toml file. Ensure those are going with Poetry\n",
    "#ensure your .env file is in the right place \n",
    "\n",
    "#make sure you're using poetry shell to get all of the dependencies going in your folder\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(os.getcwd())\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "creativeLlmModel = OpenAI(temperature=0.9)\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0006a-925a-475d-bf4a-3faf78ae322f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f340c7-4140-4569-8bb9-afa9f88723f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c95b7-591d-499b-b382-07500fffbcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da6210-ec6b-48bd-9198-e4db3709e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"model_background_personality.txt\")\n",
    "scrape_data1 = TextLoader(\"scraped_pages/example_com_combined.txt\")\n",
    "\n",
    "loaded_data = loader.load()\n",
    "scrape_data1 = scrape_data1.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56367f8-01e5-49b4-be93-71c09076e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need smaller chunks, you can enforce a maximum chunk size\n",
    "#to ensure that the splitter splits. It looks for logical places to split\n",
    "#and the .txt scrapes are jumbled and don't have logical breaks.\n",
    "\n",
    "# It may make sense to \n",
    "\n",
    "\n",
    "#Splitters to call small sections of relevant data, but it is also limiting in how to craft a creative response AFAIK\n",
    "\n",
    "# from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\\n\",\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=200,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434a3fbe-353f-4cf6-bb0d-8fecfd16d88f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_splitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_documents([loaded_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content])\n\u001b[1;32m      2\u001b[0m scrape1 \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([scrape_data1[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_splitter' is not defined"
     ]
    }
   ],
   "source": [
    "load = text_splitter.create_documents([loaded_data[0].page_content])\n",
    "scrape1 = text_splitter.create_documents([scrape_data1[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0302e62-251c-4145-ba11-56e258693f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load)\n",
    "len(scrape1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ab0c44-c783-4880-bba9-91cf43957c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW LCEL\n",
    "\n",
    "\n",
    "#User's Input > Prompt Template > LLM Model > Output Parser> Final Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63417ba-9f73-4379-99ef-8c4c45cdb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO BETTER PROMPTS READ THE E-BOOK ON PROMPTING\n",
    "\n",
    "\n",
    "#I may need to use vector databases here, or maybe not. not totally sure yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fde5872-48d3-4fba-950d-991bb6ecb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loaded_data is the personality I gave the model in the model_background_personality.txt file. I told it to be a pirate. In the live version\n",
    "#change this to the background about API3 and the product. \n",
    "\n",
    "#it is a pirate doing cold outreach selling bananas. \n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a {kind_of_entity}.\"),\n",
    "        (\"human\", \"Hello, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}{scrape_detail}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    kind_of_entity=str(load),\n",
    "   # topic=\"pirating\",\n",
    "    user_input= \"Please write an outbound cold email about the problems they may be facing without a reliable helpdesk IT support service\",\n",
    "    scrape_detail=str(scrape1),\n",
    ")\n",
    "\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887b9506-6d4d-4fb7-b333-65bea354e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# \n",
    "# NEW LCEL WAY OF DOING THINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e7f9de-494b-4a62-8607-a7118317a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b616538c-8855-45c8-bb86-e1994a266e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano Ronaldo is that he has a unique style of celebration. After scoring a goal, he often performs a signature move where he jumps into the air, spins 180 degrees, and strikes a pose upon landing. This celebration, known as the \"Siiii!\" celebration, has become iconic and is celebrated by fans worldwide. The word \"Siiii\" is a Spanish exclamation that translates to \"Yes!\" in English, and Ronaldo emphasizes this by extending his arms and shouting it as he lands. This celebration not only showcases his athleticism but also his passion for the game.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | chatModel | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eaad99-9768-4cc1-93b6-d080f02732f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85f7a773-9186-40ed-9317-00d19580265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "#examples are a way to train the model to react exactly how you want it to.\n",
    "#in the class example, this was is way to get it to speak Spanish\n",
    "#FewshotChatprompttemplate is just chatPromptTemplate with examples\n",
    "#Use this further modeling to refine the pitch, say waht works, what doesn't, etc.\n",
    "\n",
    "#train AI on how you want an outbound email to look. These are just two examples of how to train it:\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" you are only targeting Spanish speaking communities and are translating to spanish\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#a chain defines a sequence of order in which to run\n",
    "\n",
    "\n",
    "chain = final_prompt | chatModel | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb550660-4b38-4f78-a3b4-3a2d44d75dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Aquí tienes un ejemplo de un discurso de ventas para un producto hipotético, digamos, un limpiador ecológico:\n",
      "\n",
      "---\n",
      "\n",
      "¡Hola! ¿Estás cansado de usar productos de limpieza que son perjudiciales para el medio ambiente y para tu salud? Te presento nuestro limpiador ecológico \"EcoBrillo\". \n",
      "\n",
      "\"EcoBrillo\" está hecho con ingredientes 100% naturales y biodegradables, lo que significa que puedes limpiar tu hogar sin preocuparte por los químicos tóxicos que pueden afectar a tu familia y a nuestras queridas mascotas. Además, su fórmula es tan poderosa que elimina manchas difíciles y desinfecta superficies, dejando tu hogar fresco y brillante.\n",
      "\n",
      "Al elegir \"EcoBrillo\", no solo estás cuidando de tu hogar, sino también del planeta. Cada compra ayuda a financiar proyectos de reforestación. ¡Es una forma de tener un hogar limpio y contribuir a un mundo más verde!\n",
      "\n",
      "¿Te gustaría probar \"EcoBrillo\" y ver la diferencia por ti mismo? Ofrecemos una promoción especial de 20% de descuento en tu primera compra. ¡Haz de la limpieza una experiencia saludable y responsable!\n",
      "\n",
      "---\n",
      "\n",
      "¿Te gustaría más información o un enfoque diferente?\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"input\": \"please do a sales pitch to someone based on the product you sell\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32eea17-3cc9-4c03-bb6e-4b579fbc8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ PROMPTING BOOK AND DIAL THIS ALL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "475c525b-f1b9-4954-9034-728b7c67de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# FOR ADDITIONAL JSON PARSING STUFF< CHECK OUT THE PYDANTIC LIBRARY\n",
    "\n",
    "\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "# json_prompt = PromptTemplate.from_template(\n",
    "#     \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    "# )\n",
    "\n",
    "# json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2486f803-35dd-4fa8-b76a-c770b5fc16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b8a95b-5b40-4617-9367-29024f643ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_chain.invoke({\"question\": \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27939aa-c158-4b7f-bcbb-9d93b21df195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dadc2f4-08ec-4737-8f17-74fc1314fe25",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (3837994639.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    MAKE SOME GRAPHIC BASED ON WHAT THEY'RE LIKELY SPENDIN, CUSTOMIZE IT, AND SEND THEM A NEW ONE WITH HOW PCIO SOLVES THEIR PROBLEM\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e1b5d2-2123-4f04-843c-b7fcc0af4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE SOME GRAPHIC BASED ON WHAT THEY'RE LIKELY SPENDING, CUSTOMIZE IT, AND SEND THEM A NEW ONE WITH HOW PCIO SOLVES THEIR PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65f6378-03b8-4041-a1bd-9c94b2e9e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mom, research Klayvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2287bca8-9405-4595-b3aa-9d1bf0a7c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e12a893a-81c8-4b17-932d-83a0ff629b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed06f9-d74b-4e7a-8a14-437d5cb9e413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
