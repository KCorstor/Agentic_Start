{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994532eb-1413-43cd-96e0-09e1eb9d713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hubspot-api-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5436a0eb-72d9-40b3-b51c-88277c4c3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevincorstorphine/Desktop/Agentic_Start\n",
      "Loading .env file from: /Users/kevincorstorphine/Desktop/Agentic_Start/.env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "hubspot_api_key = os.getenv('HUBSPOT_API_KEY')\n",
    "\n",
    "print(os.getcwd())\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "creativeLlmModel = OpenAI(temperature=0.9)\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llmModel = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25247017-4113-432b-9d9d-5f7497df180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ASK MATT ABUOT HIS DATABASING STACK, AND HIS ACCOUNT EXECUTIVE WORKFLOW\n",
    "\n",
    "#How do you build a database\n",
    "#What tools do you use to find legit businesses\n",
    "#What tools do you use to find their contact information\n",
    "\n",
    "\n",
    "#What is your medpicc strategy?\n",
    "#How are you swarming clients?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Go through Yelp, Google, Whatever, call each and make a list,\n",
    "\n",
    "#use Linkedin to find best contacts for each business\n",
    "\n",
    "#Use Zoominfo? to find their email or contact information for every contact\n",
    "\n",
    "# Use AI to go through database, clean it, adjust for duplicates, and see what happens\n",
    "\n",
    "#Use Webscraper to write a summary for each lead after it's cleaned. Get all of this in Hubspot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500e165-e889-4849-9d62-7378917e05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Mock database, have Web Scraper go through and Scrape Every One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e6a73e-92c6-4893-b736-bd36012cfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://api3.org\n",
      "Scraping: https://api3.org/\n",
      "Scraping: https://api3.org/oev\n",
      "Scraping: https://api3.org/data-feeds\n",
      "Scraping: https://api3.org/qrng\n",
      "Scraping: https://api3.org/web3-apis\n",
      "Scraping: https://api3.org/airnode\n",
      "Scraping: https://api3.org/api3-alliance\n",
      "Scraping: https://api3.org/dao\n",
      "Scraping: https://api3.org/open-positions\n",
      "Scraping: https://api3.org/contact\n",
      "Scraping: https://api3.org/privacy-policy\n",
      "Scraping: https://api3.org/privacy-and-cookies\n",
      "Scraping: https://api3.org/terms-and-conditions\n",
      "Scraping: https://api3.org/web3-apis/Celitech Bee API\n",
      "Scraping: https://api3.org/web3-apis/Crypto API\n",
      "Scraping: https://api3.org/web3-apis/Quadency Unified API\n",
      "Scraping: https://api3.org/web3-apis/PWP Leeway\n",
      "Scraping: https://api3.org/web3-apis/Paradigm API\n",
      "Scraping: https://api3.org/web3-apis/Catallact API\n",
      "Scraping: https://api3.org/web3-apis/Bird API\n",
      "Scraping: https://api3.org/web3-apis/Asteria API\n",
      "Scraping: https://api3.org/web3-apis/Melodie API\n",
      "Scraping: https://api3.org/web3-apis/Address Verification API\n",
      "Scraping: https://api3.org/web3-apis/INFRD Insurance API\n",
      "Scraping: https://api3.org/web3-apis/Identity Authentication API\n",
      "Scraping: https://api3.org/web3-apis/The Authoritas SERP API\n",
      "Scraping: https://api3.org/web3-apis/Benzinga Market News API\n",
      "Scraping: https://api3.org/web3-apis/SMS Gateway API\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#f29a979e82b293829bc1dc9d8095\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#5d3e3233293c3e291d3c2d346e73322f3a\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#e98a86879d888a9da9889980dac7869b8e\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#94f7fbfae0f5f7e0d4f5e4fda7bafbe6f3\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#73101c1d071210073312031a405d1c0114\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#92f1fdfce6f3f1e6d2f3e2fba1bcfde0f5\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#2c5f595c5c435e586c4d5c451f024f4341\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#98fbf7f6ecf9fbecd8f9e8f1abb6f7eaff\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#56353938223735221637263f6578392431\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#9ffcf0f1ebfefcebdffeeff6acb1f0edf8\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#60030f0e1401031420011009534e0f1207\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#beddd1d0cadfddcafedfced78d90d1ccd9\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#5f2b351f3d3a31253631383e713c3032\n",
      "All pages combined and saved in: 1example_com_combined.txt\n"
     ]
    }
   ],
   "source": [
    "#Web Scraper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme in ['http', 'https'] and parsed_base_url.netloc == parsed_url.netloc\n",
    "\n",
    "def scrape_website(start_url, output_filename):\n",
    "    visited = set()\n",
    "    queue = deque([start_url])\n",
    "    combined_content = \"\"  # Variable to store all the scraped content\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "        print(f\"Scraping: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_text = soup.get_text()\n",
    "\n",
    "            # Append the content of this page to the combined_content variable\n",
    "            combined_content += f\"\\n\\n--- URL: {url} ---\\n\\n\" + page_text\n",
    "\n",
    "            # Find and queue all valid links on the page\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                link = a_tag['href']\n",
    "                absolute_link = urljoin(start_url, link)\n",
    "\n",
    "                if is_valid_url(absolute_link, start_url) and absolute_link not in visited:\n",
    "                    queue.append(absolute_link)\n",
    "\n",
    "            time.sleep(1)  # Polite scraping delay\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    \n",
    "    # Save the combined content into a single text file\n",
    "    save_combined_content(output_filename, combined_content)\n",
    "    print(f\"All pages combined and saved in: {output_filename}\")\n",
    "\n",
    "def save_combined_content(output_filename, content):\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = \"scraped_pages\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filepath = os.path.join(directory, output_filename)\n",
    "    \n",
    "    # Write all the combined content to the file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_url = \"https://api3.org\"  # Replace with the target website URL\n",
    "    output_filename = \"1example_com_combined.txt\"  # Name of the output file\n",
    "    scrape_website(website_url, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed36862-39ec-46d7-91b7-c3658dc1d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f291c0d-2152-4a9e-adb5-031da8657b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hubspot.hubspot.HubSpot object at 0x10a9ad890>\n"
     ]
    }
   ],
   "source": [
    "from hubspot import HubSpot\n",
    "\n",
    "api_client = HubSpot(access_token='hubspot_api_key')\n",
    "\n",
    "print(api_client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aa778-9fb3-4b06-b335-b92dc111910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some pushes to hubspot based on this documentation: https://pypi.org/project/hubspot-api-client/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29917e3a-3c4f-45c6-9f01-0cdb576ad23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
