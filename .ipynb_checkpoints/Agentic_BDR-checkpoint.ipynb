{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994532eb-1413-43cd-96e0-09e1eb9d713d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5436a0eb-72d9-40b3-b51c-88277c4c3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevincorstorphine/Desktop/Agentic_Start\n",
      "Loading .env file from: /Users/kevincorstorphine/Desktop/Agentic_Start/.env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "print(os.getcwd())\n",
    "dotenv_path = find_dotenv()\n",
    "print(f\"Loading .env file from: {dotenv_path}\")\n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "creativeLlmModel = OpenAI(temperature=0.9)\n",
    "chatModel = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llmModel = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e6a73e-92c6-4893-b736-bd36012cfcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://api3.org\n",
      "Scraping: https://api3.org/\n",
      "Scraping: https://api3.org/oev\n",
      "Scraping: https://api3.org/data-feeds\n",
      "Scraping: https://api3.org/qrng\n",
      "Scraping: https://api3.org/web3-apis\n",
      "Scraping: https://api3.org/airnode\n",
      "Scraping: https://api3.org/api3-alliance\n",
      "Scraping: https://api3.org/dao\n",
      "Scraping: https://api3.org/open-positions\n",
      "Scraping: https://api3.org/contact\n",
      "Scraping: https://api3.org/privacy-policy\n",
      "Scraping: https://api3.org/privacy-and-cookies\n",
      "Scraping: https://api3.org/terms-and-conditions\n",
      "Scraping: https://api3.org/web3-apis/Celitech Bee API\n",
      "Scraping: https://api3.org/web3-apis/Crypto API\n",
      "Scraping: https://api3.org/web3-apis/Quadency Unified API\n",
      "Scraping: https://api3.org/web3-apis/PWP Leeway\n",
      "Scraping: https://api3.org/web3-apis/Paradigm API\n",
      "Scraping: https://api3.org/web3-apis/Catallact API\n",
      "Scraping: https://api3.org/web3-apis/Bird API\n",
      "Scraping: https://api3.org/web3-apis/Asteria API\n",
      "Scraping: https://api3.org/web3-apis/Melodie API\n",
      "Scraping: https://api3.org/web3-apis/Address Verification API\n",
      "Scraping: https://api3.org/web3-apis/INFRD Insurance API\n",
      "Scraping: https://api3.org/web3-apis/Identity Authentication API\n",
      "Scraping: https://api3.org/web3-apis/The Authoritas SERP API\n",
      "Scraping: https://api3.org/web3-apis/Benzinga Market News API\n",
      "Scraping: https://api3.org/web3-apis/SMS Gateway API\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#5e363b322e1e3f2e376d70312c39\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#72111d1c061311063213021b415c1d0015\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#ea8985849e8b899eaa8b9a83d9c485988d\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#52313d3c263331261233223b617c3d2035\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#781b17160c191b0c381908114b56170a1f\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#97f4f8f9e3f6f4e3d7f6e7fea4b9f8e5f0\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#7b080e0b0b14090f3b1a0b124855181416\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#c4a7abaab0a5a7b084a5b4adf7eaabb6a3\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#ceada1a0baafadba8eafbea7fde0a1bca9\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#32515d5c465351467253425b011c5d4055\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#60030f0e1401031420011009534e0f1207\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#492a26273d282a3d092839207a67263b2e\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#593a36372d383a2d193829306a77362b3e\n",
      "Scraping: https://api3.org/cdn-cgi/l/email-protection#295d43694b4c475340474e48074a4644\n",
      "All pages combined and saved in: example_com_combined.txt\n"
     ]
    }
   ],
   "source": [
    "#Web Scraper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def is_valid_url(url, base_url):\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.scheme in ['http', 'https'] and parsed_base_url.netloc == parsed_url.netloc\n",
    "\n",
    "def scrape_website(start_url, output_filename):\n",
    "    visited = set()\n",
    "    queue = deque([start_url])\n",
    "    combined_content = \"\"  # Variable to store all the scraped content\n",
    "\n",
    "    while queue:\n",
    "        url = queue.popleft()\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(url)\n",
    "        print(f\"Scraping: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_text = soup.get_text()\n",
    "\n",
    "            # Append the content of this page to the combined_content variable\n",
    "            combined_content += f\"\\n\\n--- URL: {url} ---\\n\\n\" + page_text\n",
    "\n",
    "            # Find and queue all valid links on the page\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                link = a_tag['href']\n",
    "                absolute_link = urljoin(start_url, link)\n",
    "\n",
    "                if is_valid_url(absolute_link, start_url) and absolute_link not in visited:\n",
    "                    queue.append(absolute_link)\n",
    "\n",
    "            time.sleep(1)  # Polite scraping delay\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    \n",
    "    # Save the combined content into a single text file\n",
    "    save_combined_content(output_filename, combined_content)\n",
    "    print(f\"All pages combined and saved in: {output_filename}\")\n",
    "\n",
    "def save_combined_content(output_filename, content):\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = \"scraped_pages\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    filepath = os.path.join(directory, output_filename)\n",
    "    \n",
    "    # Write all the combined content to the file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    website_url = \"https://api3.org\"  # Replace with the target website URL\n",
    "    output_filename = \"1example_com_combined.txt\"  # Name of the output file\n",
    "    scrape_website(website_url, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed36862-39ec-46d7-91b7-c3658dc1d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
